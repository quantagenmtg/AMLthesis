{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from MainFiles.GP import *\n",
    "from Windowing import *\n",
    "import torch\n",
    "from Windowing import *\n",
    "from torch.linalg import inv, det\n",
    "import torch.distributions as tdb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "We now try the original thing again but with the better cov_kernel function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "            Count\nsize_train       \n16            245\n23            246\n32            246\n40              1\n45            244\n...           ...\n1048576         2\n1482910         1\n1486391         1\n2097152         1\n2205023         1\n\n[201 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Count</th>\n    </tr>\n    <tr>\n      <th>size_train</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>16</th>\n      <td>245</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>246</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>246</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>244</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1048576</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1482910</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1486391</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2097152</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2205023</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>201 rows Ã— 1 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "win = Windowing(aggCurves)\n",
    "win.set_points([16,   23,   32,   45,   64,   91,  128,  181,  256,  362,  512,\n",
    "        724, 1024, 1448, 2048, 2896])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "y = torch.tensor(win.data, dtype=torch.float)\n",
    "X = torch.tensor(win.train_anchors, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def optimize_params(params, y, X, lr = 0.001, n_iter = 100):\n",
    "    '''\n",
    "    Optimise the hyperparameters for the GP\n",
    "\n",
    "    :param params: initial parameters for the covariance kernel and mean function\n",
    "    :param y: actual outputs\n",
    "    :param X: training anchors\n",
    "    :param lr: learning rate\n",
    "    :param n_iter: number of iterations\n",
    "    :return: optimised parameters\n",
    "    '''\n",
    "    params = params.clone().requires_grad_(True)\n",
    "    optimizer = torch.optim.AdamW([params], lr=lr)\n",
    "    gr = torch.ones(params.shape[1:])\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        optimizer.zero_grad()\n",
    "        loss = -log_marginal_likelihood(params, y, X)\n",
    "        loss.backward(gradient=gr)\n",
    "        optimizer.step()\n",
    "        print(f'Iteration {i} marginal likelihood: {np.exp(-loss.mean().item())}')\n",
    "    return params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 marginal likelihood: 0.684665147298796\n",
      "Iteration 1 marginal likelihood: 0.881970839894196\n",
      "Iteration 2 marginal likelihood: 0.9518115682244522\n",
      "Iteration 3 marginal likelihood: 0.9733848866226816\n",
      "Iteration 4 marginal likelihood: 0.979259820074013\n",
      "Iteration 5 marginal likelihood: 0.9804559537613687\n",
      "Iteration 6 marginal likelihood: 0.9803873990512985\n",
      "Iteration 7 marginal likelihood: 0.9797502614019555\n",
      "Iteration 8 marginal likelihood: 0.9788407860953964\n",
      "Iteration 9 marginal likelihood: 0.9783258054037545\n",
      "Iteration 10 marginal likelihood: 0.9774074517115334\n",
      "Iteration 11 marginal likelihood: 0.9776413310383262\n",
      "Iteration 12 marginal likelihood: 0.9756077361583094\n",
      "Iteration 13 marginal likelihood: 0.9776278648443304\n",
      "Iteration 14 marginal likelihood: 0.9763428785901629\n",
      "Iteration 15 marginal likelihood: 0.9768963603006758\n",
      "Iteration 16 marginal likelihood: 0.9772512653268361\n",
      "Iteration 17 marginal likelihood: 0.9775345243625527\n",
      "Iteration 18 marginal likelihood: 0.9778180093865989\n",
      "Iteration 19 marginal likelihood: 0.9780383657208463\n",
      "Iteration 20 marginal likelihood: 0.9783840561203673\n",
      "Iteration 21 marginal likelihood: 0.9788068709019133\n",
      "Iteration 22 marginal likelihood: 0.979330238649032\n",
      "Iteration 23 marginal likelihood: 0.9799477603135821\n",
      "Iteration 24 marginal likelihood: 0.9806213137858523\n",
      "Iteration 25 marginal likelihood: 0.9813728907019246\n",
      "Iteration 26 marginal likelihood: 0.9821117736300701\n",
      "Iteration 27 marginal likelihood: 0.9828135853185711\n",
      "Iteration 28 marginal likelihood: 0.9836068329507409\n",
      "Iteration 29 marginal likelihood: 0.9843021850078837\n",
      "Iteration 30 marginal likelihood: 0.9849433101766725\n",
      "Iteration 31 marginal likelihood: 0.9855200533503445\n",
      "Iteration 32 marginal likelihood: 0.9860294257311356\n",
      "Iteration 33 marginal likelihood: 0.9864648813837346\n",
      "Iteration 34 marginal likelihood: 0.9868329366156257\n",
      "Iteration 35 marginal likelihood: 0.9871330212491632\n",
      "Iteration 36 marginal likelihood: 0.9873814276306475\n",
      "Iteration 37 marginal likelihood: 0.9875753372131303\n",
      "Iteration 38 marginal likelihood: 0.9877305473071735\n",
      "Iteration 39 marginal likelihood: 0.9878604994097311\n",
      "Iteration 40 marginal likelihood: 0.9879689633823733\n",
      "Iteration 41 marginal likelihood: 0.9880576050665453\n",
      "Iteration 42 marginal likelihood: 0.9881381902974807\n",
      "Iteration 43 marginal likelihood: 0.9882114312890986\n",
      "Iteration 44 marginal likelihood: 0.9882875042981264\n",
      "Iteration 45 marginal likelihood: 0.9883383925626482\n",
      "Iteration 46 marginal likelihood: 0.9884262766450622\n",
      "Iteration 47 marginal likelihood: 0.9884943918617636\n",
      "Iteration 48 marginal likelihood: 0.9885604826165911\n",
      "Iteration 49 marginal likelihood: 0.9886146562468537\n"
     ]
    }
   ],
   "source": [
    "params = torch.tensor([0.5,0.5,0.5, 0.5], dtype=torch.float32)\n",
    "shp = np.hstack((np.array(y.shape[:2]), np.array(y.shape[3])))\n",
    "params = params.repeat(*shp, 1)\n",
    "\n",
    "# params at front\n",
    "params = params.permute(*np.roll(np.arange(len(params.shape)),1))\n",
    "\n",
    "params = optimize_params(params, y, X, lr = 0.1, n_iter = 50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Now for one curve\n",
    "Redefine some stuff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "def log_marginal_likelihood(params, y, X):\n",
    "    '''\n",
    "    Log marginal likelihood for optimization of hyperparameters for GP\n",
    "\n",
    "    :param params: arameters for the covariance kernel and mean function\n",
    "    :param y: actual outputs\n",
    "    :param X: training anchors\n",
    "    :return: log marginal likelihood\n",
    "    '''\n",
    "    # set mean\n",
    "    mean = params[0]\n",
    "\n",
    "    # make nans 0\n",
    "    y = y - mean\n",
    "    # nan = torch.isnan(y)\n",
    "    # nanT = nan.transpose(-1, -2)\n",
    "    # nan_ind = torch.where(nanT)\n",
    "    # y = torch.where(nan, torch.tensor(0.0), y)\n",
    "    # yT = y.transpose(-1, -2)\n",
    "\n",
    "    # get covariance matrix and set rows/columns where nan to 0 as these are technically not datapoints\n",
    "    cov = cov_kernel(X, X[..., None], params[1:])\n",
    "    # cov[nanT, :] = 0\n",
    "    # cov.transpose(-1, -2)[nanT, :] = 0\n",
    "    # cov[nan_ind[0], nan_ind[1], nan_ind[2], nan_ind[3], nan_ind[\n",
    "    #     3]] = 1  # The trick is to set the \"fake\" datapoints diagonal to 1 and rest of its row and column to 1,\n",
    "    # # this way it is like it's not there for the determinant and inverse\n",
    "    inv_cov = inv(cov)\n",
    "\n",
    "    # calculate the matrix multiplication\n",
    "    matmul = torch.einsum('...i,...ij,...j', y, inv_cov, y)\n",
    "\n",
    "    res = -1 / 2 * matmul\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def optimize_params(params, y, X, lr = 0.001, n_iter = 100):\n",
    "    '''\n",
    "    Optimise the hyperparameters for the GP\n",
    "\n",
    "    :param params: initial parameters for the covariance kernel and mean function\n",
    "    :param y: actual outputs\n",
    "    :param X: training anchors\n",
    "    :param lr: learning rate\n",
    "    :param n_iter: number of iterations\n",
    "    :return: optimised parameters\n",
    "    '''\n",
    "    params = params.clone().requires_grad_(True)\n",
    "    optimizer = torch.optim.AdamW([params], lr=lr)\n",
    "    # gr = torch.ones(params.shape[1:])\n",
    "\n",
    "    losses = []\n",
    "    for i in range(n_iter):\n",
    "        optimizer.zero_grad()\n",
    "        loss = -log_marginal_likelihood(params, y, X)\n",
    "        losses.append(loss.detach().numpy())\n",
    "        if i % 10 == 0:\n",
    "            print(f'Iteration {i} loss: {loss.item()}')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if len(losses) > 10:\n",
    "            if np.mean(losses[-10:]) == losses[-1]:\n",
    "                print(\"Stopped at iteration: \",i)\n",
    "                break\n",
    "\n",
    "    return params, losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "cut = y[[0,1],0,:,-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss: 2.2514023780822754\n",
      "Iteration 10 loss: 0.8274351358413696\n",
      "Iteration 20 loss: 0.3571223318576813\n",
      "Iteration 30 loss: 0.196620374917984\n",
      "Iteration 40 loss: 0.13437102735042572\n",
      "Iteration 50 loss: 0.10606352984905243\n",
      "Iteration 60 loss: 0.09099937975406647\n",
      "Iteration 70 loss: 0.08180364966392517\n",
      "Iteration 80 loss: 0.07555307447910309\n",
      "Iteration 90 loss: 0.07096251100301743\n",
      "Iteration 100 loss: 0.06740568578243256\n",
      "Iteration 110 loss: 0.06454425305128098\n",
      "Iteration 120 loss: 0.06217731535434723\n",
      "Iteration 130 loss: 0.06017540395259857\n",
      "Iteration 140 loss: 0.05844971165060997\n",
      "Iteration 150 loss: 0.05693645775318146\n",
      "Iteration 160 loss: 0.05558842048048973\n",
      "Iteration 170 loss: 0.05436985567212105\n",
      "Iteration 180 loss: 0.053253449499607086\n",
      "Iteration 190 loss: 0.052218277007341385\n",
      "Iteration 200 loss: 0.051248252391815186\n",
      "Iteration 210 loss: 0.05033114552497864\n",
      "Iteration 220 loss: 0.04945765435695648\n",
      "Iteration 230 loss: 0.0486207976937294\n",
      "Iteration 240 loss: 0.04781532287597656\n",
      "Iteration 250 loss: 0.047037363052368164\n",
      "Iteration 260 loss: 0.0462840311229229\n",
      "Iteration 270 loss: 0.045553117990493774\n",
      "Iteration 280 loss: 0.044842999428510666\n",
      "Iteration 290 loss: 0.04482085257768631\n",
      "Iteration 300 loss: 0.04481533169746399\n",
      "Iteration 310 loss: 0.04480963572859764\n",
      "Iteration 320 loss: 0.044803619384765625\n",
      "Iteration 330 loss: 0.044797249138355255\n",
      "Iteration 340 loss: 0.04479055851697922\n",
      "Iteration 350 loss: 0.04478362947702408\n",
      "Iteration 360 loss: 0.04477648064494133\n",
      "Iteration 370 loss: 0.044769205152988434\n",
      "Iteration 380 loss: 0.04476180300116539\n",
      "Iteration 390 loss: 0.04475516825914383\n",
      "Iteration 400 loss: 0.044746845960617065\n",
      "Iteration 410 loss: 0.04473932832479477\n",
      "Iteration 420 loss: 0.04473179578781128\n",
      "Iteration 430 loss: 0.04472425952553749\n",
      "Iteration 440 loss: 0.044716738164424896\n",
      "Iteration 450 loss: 0.0447092279791832\n",
      "Iteration 460 loss: 0.0447017103433609\n",
      "Iteration 470 loss: 0.04469702020287514\n",
      "Iteration 480 loss: 0.04468671232461929\n",
      "Iteration 490 loss: 0.04467920958995819\n",
      "Iteration 500 loss: 0.044675055891275406\n",
      "Iteration 510 loss: 0.04466421902179718\n",
      "Iteration 520 loss: 0.044656723737716675\n",
      "Iteration 530 loss: 0.044649213552474976\n",
      "Iteration 540 loss: 0.044641707092523575\n",
      "Iteration 550 loss: 0.04463418945670128\n",
      "Iteration 560 loss: 0.04462667182087898\n",
      "Iteration 570 loss: 0.0446191243827343\n",
      "Iteration 580 loss: 0.04461405798792839\n",
      "Iteration 590 loss: 0.04460400342941284\n",
      "Iteration 600 loss: 0.04459641873836517\n",
      "Iteration 610 loss: 0.044588811695575714\n",
      "Iteration 620 loss: 0.04458119347691536\n",
      "Iteration 630 loss: 0.04457354545593262\n",
      "Iteration 640 loss: 0.04456587880849838\n",
      "Iteration 650 loss: 0.04455820098519325\n",
      "Iteration 660 loss: 0.04455049708485603\n",
      "Iteration 670 loss: 0.044542767107486725\n",
      "Iteration 680 loss: 0.04453500360250473\n",
      "Iteration 690 loss: 0.044527243822813034\n",
      "Iteration 700 loss: 0.044519443064928055\n",
      "Iteration 710 loss: 0.044511616230010986\n",
      "Iteration 720 loss: 0.044503770768642426\n",
      "Iteration 730 loss: 0.04449588805437088\n",
      "Iteration 740 loss: 0.04448798671364784\n",
      "Iteration 750 loss: 0.04448005557060242\n",
      "Iteration 760 loss: 0.0444721020758152\n",
      "Iteration 770 loss: 0.04446413367986679\n",
      "Iteration 780 loss: 0.0444561205804348\n",
      "Iteration 790 loss: 0.044448088854551315\n",
      "Iteration 800 loss: 0.044440027326345444\n",
      "Iteration 810 loss: 0.04443195462226868\n",
      "Iteration 820 loss: 0.044423844665288925\n",
      "Iteration 830 loss: 0.044415704905986786\n",
      "Iteration 840 loss: 0.04440755024552345\n",
      "Iteration 850 loss: 0.044399362057447433\n",
      "Iteration 860 loss: 0.04439115896821022\n",
      "Iteration 870 loss: 0.04438292235136032\n",
      "Iteration 880 loss: 0.044374678283929825\n",
      "Iteration 890 loss: 0.04436638951301575\n",
      "Iteration 900 loss: 0.044358085840940475\n",
      "Iteration 910 loss: 0.04434974864125252\n",
      "Iteration 920 loss: 0.04434140399098396\n",
      "Iteration 930 loss: 0.04433303698897362\n",
      "Iteration 940 loss: 0.04432462900876999\n",
      "Iteration 950 loss: 0.044316209852695465\n",
      "Iteration 960 loss: 0.04431033506989479\n",
      "Iteration 970 loss: 0.04429930821061134\n",
      "Iteration 980 loss: 0.044290825724601746\n",
      "Iteration 990 loss: 0.04428231343626976\n",
      "Iteration 1000 loss: 0.04427379369735718\n",
      "Iteration 1010 loss: 0.04426523670554161\n",
      "Iteration 1020 loss: 0.04425667226314545\n",
      "Iteration 1030 loss: 0.04424808546900749\n",
      "Iteration 1040 loss: 0.04423949122428894\n",
      "Iteration 1050 loss: 0.044230867177248\n",
      "Iteration 1060 loss: 0.04422222077846527\n",
      "Iteration 1070 loss: 0.044213563203811646\n",
      "Iteration 1080 loss: 0.044204890727996826\n",
      "Iteration 1090 loss: 0.044196199625730515\n",
      "Iteration 1100 loss: 0.04418748989701271\n",
      "Iteration 1110 loss: 0.04417875409126282\n",
      "Iteration 1120 loss: 0.04417002201080322\n",
      "Iteration 1130 loss: 0.04416126012802124\n",
      "Iteration 1140 loss: 0.044152483344078064\n",
      "Iteration 1150 loss: 0.044143691658973694\n",
      "Iteration 1160 loss: 0.04413488507270813\n",
      "Iteration 1170 loss: 0.04412606731057167\n",
      "Iteration 1180 loss: 0.04411722347140312\n",
      "Iteration 1190 loss: 0.04411053657531738\n",
      "Iteration 1200 loss: 0.04409952834248543\n",
      "Iteration 1210 loss: 0.04409065097570419\n",
      "Iteration 1220 loss: 0.04408176243305206\n",
      "Iteration 1230 loss: 0.044072866439819336\n",
      "Iteration 1240 loss: 0.044063959270715714\n",
      "Iteration 1250 loss: 0.044055044651031494\n",
      "Iteration 1260 loss: 0.04404610022902489\n",
      "Iteration 1270 loss: 0.04403715953230858\n",
      "Iteration 1280 loss: 0.04402819648385048\n",
      "Iteration 1290 loss: 0.04401922598481178\n",
      "Iteration 1300 loss: 0.044010259211063385\n",
      "Iteration 1310 loss: 0.044001273810863495\n",
      "Iteration 1320 loss: 0.04399226978421211\n",
      "Iteration 1330 loss: 0.043983276933431625\n",
      "Iteration 1340 loss: 0.043974246829748154\n",
      "Iteration 1350 loss: 0.043965231627225876\n",
      "Iteration 1360 loss: 0.04395619407296181\n",
      "Iteration 1370 loss: 0.04394714534282684\n",
      "Iteration 1380 loss: 0.04393810033798218\n",
      "Iteration 1390 loss: 0.04392903298139572\n",
      "Iteration 1400 loss: 0.04391997680068016\n",
      "Iteration 1410 loss: 0.04391089826822281\n",
      "Iteration 1420 loss: 0.04390181228518486\n",
      "Iteration 1430 loss: 0.04389720410108566\n",
      "Iteration 1440 loss: 0.04388362914323807\n",
      "Iteration 1450 loss: 0.043874531984329224\n",
      "Iteration 1460 loss: 0.04386541619896889\n",
      "Iteration 1470 loss: 0.043856292963027954\n",
      "Iteration 1480 loss: 0.043847184628248215\n",
      "Iteration 1490 loss: 0.043838050216436386\n",
      "Iteration 1500 loss: 0.043828919529914856\n",
      "Iteration 1510 loss: 0.043819766491651535\n",
      "Iteration 1520 loss: 0.043810635805130005\n",
      "Iteration 1530 loss: 0.04380694776773453\n",
      "Iteration 1540 loss: 0.04379231855273247\n",
      "Iteration 1550 loss: 0.04378316551446915\n",
      "Iteration 1560 loss: 0.04377400875091553\n",
      "Iteration 1570 loss: 0.043764833360910416\n",
      "Iteration 1580 loss: 0.043755657970905304\n",
      "Iteration 1590 loss: 0.04374648630619049\n",
      "Iteration 1600 loss: 0.043742548674345016\n",
      "Iteration 1610 loss: 0.04372812435030937\n",
      "Iteration 1620 loss: 0.043718934059143066\n",
      "Iteration 1630 loss: 0.043709732592105865\n",
      "Iteration 1640 loss: 0.043700527399778366\n",
      "Iteration 1650 loss: 0.04369133710861206\n",
      "Iteration 1660 loss: 0.04368213191628456\n",
      "Iteration 1670 loss: 0.04367292672395706\n",
      "Iteration 1680 loss: 0.043663717806339264\n",
      "Iteration 1690 loss: 0.04365450516343117\n",
      "Iteration 1700 loss: 0.04364529252052307\n",
      "Iteration 1710 loss: 0.04363827407360077\n",
      "Iteration 1720 loss: 0.043626848608255386\n",
      "Iteration 1730 loss: 0.04361763596534729\n",
      "Iteration 1740 loss: 0.0436084121465683\n",
      "Iteration 1750 loss: 0.04359918087720871\n",
      "Iteration 1760 loss: 0.043589960783720016\n",
      "Iteration 1770 loss: 0.04358072951436043\n",
      "Iteration 1780 loss: 0.04357149824500084\n",
      "Iteration 1790 loss: 0.043562255799770355\n",
      "Iteration 1800 loss: 0.043553031980991364\n",
      "Iteration 1810 loss: 0.043543796986341476\n",
      "Iteration 1820 loss: 0.04353455454111099\n",
      "Iteration 1830 loss: 0.04352531582117081\n",
      "Iteration 1840 loss: 0.04351608082652092\n",
      "Iteration 1850 loss: 0.04350683465600014\n",
      "Iteration 1860 loss: 0.04349759221076965\n",
      "Iteration 1870 loss: 0.04348834231495857\n",
      "Iteration 1880 loss: 0.043479107320308685\n",
      "Iteration 1890 loss: 0.0434698648750782\n",
      "Iteration 1900 loss: 0.04346060752868652\n",
      "Iteration 1910 loss: 0.04345136508345604\n",
      "Iteration 1920 loss: 0.043442122638225555\n",
      "Iteration 1930 loss: 0.043432869017124176\n",
      "Iteration 1940 loss: 0.04342362657189369\n",
      "Iteration 1950 loss: 0.04341437295079231\n",
      "Iteration 1960 loss: 0.04340513050556183\n",
      "Iteration 1970 loss: 0.04339587315917015\n",
      "Iteration 1980 loss: 0.04338662326335907\n",
      "Iteration 1990 loss: 0.043377384543418884\n",
      "Iteration 2000 loss: 0.043368127197027206\n",
      "Iteration 2010 loss: 0.043358877301216125\n",
      "Iteration 2020 loss: 0.04334963485598564\n",
      "Iteration 2030 loss: 0.04334038868546486\n",
      "Iteration 2040 loss: 0.04333112761378288\n",
      "Iteration 2050 loss: 0.0433218814432621\n",
      "Iteration 2060 loss: 0.04331262782216072\n",
      "Iteration 2070 loss: 0.043303392827510834\n",
      "Iteration 2080 loss: 0.043294135481119156\n",
      "Iteration 2090 loss: 0.043284885585308075\n",
      "Iteration 2100 loss: 0.04327564686536789\n",
      "Iteration 2110 loss: 0.04326639696955681\n",
      "Iteration 2120 loss: 0.043257154524326324\n",
      "Iteration 2130 loss: 0.043247904628515244\n",
      "Iteration 2140 loss: 0.04323865845799446\n",
      "Iteration 2150 loss: 0.04322941228747368\n",
      "Iteration 2160 loss: 0.043220166116952896\n",
      "Iteration 2170 loss: 0.04321092739701271\n",
      "Iteration 2180 loss: 0.043201688677072525\n",
      "Iteration 2190 loss: 0.04319244623184204\n",
      "Iteration 2200 loss: 0.04318320378661156\n",
      "Iteration 2210 loss: 0.043173957616090775\n",
      "Iteration 2220 loss: 0.04316471889615059\n",
      "Iteration 2230 loss: 0.043155487626791\n",
      "Iteration 2240 loss: 0.04314623400568962\n",
      "Iteration 2250 loss: 0.04313700273633003\n",
      "Iteration 2260 loss: 0.043127767741680145\n",
      "Iteration 2270 loss: 0.04311853274703026\n",
      "Iteration 2280 loss: 0.043109290301799774\n",
      "Iteration 2290 loss: 0.04310005530714989\n",
      "Iteration 2300 loss: 0.0430908240377903\n",
      "Iteration 2310 loss: 0.04308159276843071\n",
      "Iteration 2320 loss: 0.04307236149907112\n",
      "Iteration 2330 loss: 0.04306313395500183\n",
      "Iteration 2340 loss: 0.043053895235061646\n",
      "Iteration 2350 loss: 0.04304467886686325\n",
      "Iteration 2360 loss: 0.043035443872213364\n",
      "Iteration 2370 loss: 0.04302623122930527\n",
      "Iteration 2380 loss: 0.04301699623465538\n",
      "Iteration 2390 loss: 0.04300803691148758\n",
      "Iteration 2400 loss: 0.042998552322387695\n",
      "Iteration 2410 loss: 0.042989332228899\n",
      "Iteration 2420 loss: 0.04298010468482971\n",
      "Iteration 2430 loss: 0.04297088831663132\n",
      "Iteration 2440 loss: 0.04296167194843292\n",
      "Iteration 2450 loss: 0.04295244812965393\n",
      "Iteration 2460 loss: 0.042943235486745834\n",
      "Iteration 2470 loss: 0.04293401911854744\n",
      "Iteration 2480 loss: 0.04292481020092964\n",
      "Iteration 2490 loss: 0.042915597558021545\n",
      "Iteration 2500 loss: 0.042906392365694046\n",
      "Iteration 2510 loss: 0.04289717227220535\n",
      "Iteration 2520 loss: 0.04288797080516815\n",
      "Iteration 2530 loss: 0.042878758162260056\n",
      "Iteration 2540 loss: 0.04286954924464226\n",
      "Iteration 2550 loss: 0.04286034032702446\n",
      "Iteration 2560 loss: 0.042852919548749924\n",
      "Iteration 2570 loss: 0.04284193739295006\n",
      "Iteration 2580 loss: 0.042832743376493454\n",
      "Iteration 2590 loss: 0.04282354563474655\n",
      "Iteration 2600 loss: 0.04281434416770935\n",
      "Iteration 2610 loss: 0.04280513897538185\n",
      "Iteration 2620 loss: 0.0427984893321991\n",
      "Iteration 2630 loss: 0.04278673976659775\n",
      "Iteration 2640 loss: 0.04277755692601204\n",
      "Iteration 2650 loss: 0.04276835918426514\n",
      "Iteration 2660 loss: 0.042759161442518234\n",
      "Iteration 2670 loss: 0.04274997487664223\n",
      "Iteration 2680 loss: 0.042740777134895325\n",
      "Iteration 2690 loss: 0.04273158684372902\n",
      "Iteration 2700 loss: 0.042722392827272415\n",
      "Iteration 2710 loss: 0.0427132174372673\n",
      "Iteration 2720 loss: 0.042704027146101\n",
      "Iteration 2730 loss: 0.04269484803080559\n",
      "Iteration 2740 loss: 0.04268566519021988\n",
      "Iteration 2750 loss: 0.04267647862434387\n",
      "Iteration 2760 loss: 0.04266729578375816\n",
      "Iteration 2770 loss: 0.04265811666846275\n",
      "Iteration 2780 loss: 0.04264894127845764\n",
      "Iteration 2790 loss: 0.04263976216316223\n",
      "Iteration 2800 loss: 0.04263058304786682\n",
      "Iteration 2810 loss: 0.04262140765786171\n",
      "Iteration 2820 loss: 0.0426122322678566\n",
      "Iteration 2830 loss: 0.04260306432843208\n",
      "Iteration 2840 loss: 0.04259388521313667\n",
      "Iteration 2850 loss: 0.04258471727371216\n",
      "Iteration 2860 loss: 0.042575541883707047\n",
      "Iteration 2870 loss: 0.042566388845443726\n",
      "Iteration 2880 loss: 0.042557213455438614\n",
      "Iteration 2890 loss: 0.0425480455160141\n",
      "Iteration 2900 loss: 0.04253888502717018\n",
      "Iteration 2910 loss: 0.04252971336245537\n",
      "Iteration 2920 loss: 0.042520541697740555\n",
      "Iteration 2930 loss: 0.042511384934186935\n",
      "Iteration 2940 loss: 0.042502228170633316\n",
      "Iteration 2950 loss: 0.0424930602312088\n",
      "Iteration 2960 loss: 0.04248390346765518\n",
      "Iteration 2970 loss: 0.042474739253520966\n",
      "Iteration 2980 loss: 0.042465582489967346\n",
      "Iteration 2990 loss: 0.04245642572641373\n",
      "Iteration 3000 loss: 0.04244726523756981\n",
      "Iteration 3010 loss: 0.04243811219930649\n",
      "Iteration 3020 loss: 0.04242895916104317\n",
      "Iteration 3030 loss: 0.04241980239748955\n",
      "Iteration 3040 loss: 0.04241064935922623\n",
      "Iteration 3050 loss: 0.04240149259567261\n",
      "Iteration 3060 loss: 0.04239233210682869\n",
      "Iteration 3070 loss: 0.04238319396972656\n",
      "Iteration 3080 loss: 0.04237404093146324\n",
      "Iteration 3090 loss: 0.042364880442619324\n",
      "Iteration 3100 loss: 0.042355746030807495\n",
      "Iteration 3110 loss: 0.042346589267253876\n",
      "Iteration 3120 loss: 0.04233744740486145\n",
      "Iteration 3130 loss: 0.04232829809188843\n",
      "Iteration 3140 loss: 0.042319148778915405\n",
      "Iteration 3150 loss: 0.04231000691652298\n",
      "Iteration 3160 loss: 0.04230085760354996\n",
      "Iteration 3170 loss: 0.04229171201586723\n",
      "Iteration 3180 loss: 0.04228256270289421\n",
      "Iteration 3190 loss: 0.042273424565792084\n",
      "Iteration 3200 loss: 0.04226427525281906\n",
      "Iteration 3210 loss: 0.04225514084100723\n",
      "Iteration 3220 loss: 0.04224599152803421\n",
      "Iteration 3230 loss: 0.04223686084151268\n",
      "Iteration 3240 loss: 0.04222772270441055\n",
      "Iteration 3250 loss: 0.04221857339143753\n",
      "Iteration 3260 loss: 0.042209431529045105\n",
      "Iteration 3270 loss: 0.042200300842523575\n",
      "Iteration 3280 loss: 0.04219115898013115\n",
      "Iteration 3290 loss: 0.04218203201889992\n",
      "Iteration 3300 loss: 0.04217289015650749\n",
      "Iteration 3310 loss: 0.042163748294115067\n",
      "Iteration 3320 loss: 0.04215461388230324\n",
      "Iteration 3330 loss: 0.04214547574520111\n",
      "Iteration 3340 loss: 0.04213634133338928\n",
      "Iteration 3350 loss: 0.04212721437215805\n",
      "Iteration 3360 loss: 0.04211807623505592\n",
      "Iteration 3370 loss: 0.0421089231967926\n",
      "Iteration 3380 loss: 0.04209980368614197\n",
      "Iteration 3390 loss: 0.04209067299962044\n",
      "Iteration 3400 loss: 0.04208153486251831\n",
      "Iteration 3410 loss: 0.042072393000125885\n",
      "Iteration 3420 loss: 0.04206326603889465\n",
      "Iteration 3430 loss: 0.04205413535237312\n",
      "Iteration 3440 loss: 0.042045000940561295\n",
      "Iteration 3450 loss: 0.042035870254039764\n",
      "Iteration 3460 loss: 0.04202672839164734\n",
      "Iteration 3470 loss: 0.04201760143041611\n",
      "Iteration 3480 loss: 0.04200847074389458\n",
      "Iteration 3490 loss: 0.04199933260679245\n",
      "Iteration 3500 loss: 0.04199019446969032\n",
      "Iteration 3510 loss: 0.04198107495903969\n",
      "Iteration 3520 loss: 0.04197193309664726\n",
      "Iteration 3530 loss: 0.04196280986070633\n",
      "Iteration 3540 loss: 0.0419536754488945\n",
      "Iteration 3550 loss: 0.04194454476237297\n",
      "Iteration 3560 loss: 0.041935406625270844\n",
      "Iteration 3570 loss: 0.04192628338932991\n",
      "Iteration 3580 loss: 0.04191715642809868\n",
      "Iteration 3590 loss: 0.04190802201628685\n",
      "Iteration 3600 loss: 0.04189888387918472\n",
      "Iteration 3610 loss: 0.041894614696502686\n",
      "Iteration 3620 loss: 0.04188062995672226\n",
      "Iteration 3630 loss: 0.04187149554491043\n",
      "Iteration 3640 loss: 0.0418623648583889\n",
      "Iteration 3650 loss: 0.04185323044657707\n",
      "Iteration 3660 loss: 0.04184408858418465\n",
      "Iteration 3670 loss: 0.041834961622953415\n",
      "Iteration 3680 loss: 0.041825827211141586\n",
      "Iteration 3690 loss: 0.041816696524620056\n",
      "Iteration 3700 loss: 0.04180756211280823\n",
      "Iteration 3710 loss: 0.0417984314262867\n",
      "Iteration 3720 loss: 0.041789304465055466\n",
      "Iteration 3730 loss: 0.04178017005324364\n",
      "Iteration 3740 loss: 0.04177103191614151\n",
      "Iteration 3750 loss: 0.04176189750432968\n",
      "Iteration 3760 loss: 0.04175276681780815\n",
      "Iteration 3770 loss: 0.041743628680706024\n",
      "Iteration 3780 loss: 0.04173450171947479\n",
      "Iteration 3790 loss: 0.04172535985708237\n",
      "Iteration 3800 loss: 0.041716232895851135\n",
      "Iteration 3810 loss: 0.04170709848403931\n",
      "Iteration 3820 loss: 0.04169795662164688\n",
      "Iteration 3830 loss: 0.04168882220983505\n",
      "Iteration 3840 loss: 0.041679684072732925\n",
      "Iteration 3850 loss: 0.041670557111501694\n",
      "Iteration 3860 loss: 0.04166141524910927\n",
      "Iteration 3870 loss: 0.04165227711200714\n",
      "Iteration 3880 loss: 0.04164312779903412\n",
      "Iteration 3890 loss: 0.04163399711251259\n",
      "Iteration 3900 loss: 0.04162486270070076\n",
      "Iteration 3910 loss: 0.04161570966243744\n",
      "Iteration 3920 loss: 0.041608184576034546\n",
      "Iteration 3930 loss: 0.04159743711352348\n",
      "Iteration 3940 loss: 0.041588302701711655\n",
      "Iteration 3950 loss: 0.04157915338873863\n",
      "Iteration 3960 loss: 0.041570018976926804\n",
      "Iteration 3970 loss: 0.041560880839824677\n",
      "Iteration 3980 loss: 0.041551731526851654\n",
      "Iteration 3990 loss: 0.04154258966445923\n",
      "Iteration 4000 loss: 0.041533444076776505\n",
      "Iteration 4010 loss: 0.04152429848909378\n",
      "Iteration 4020 loss: 0.04151514545083046\n",
      "Iteration 4030 loss: 0.041505999863147736\n",
      "Iteration 4040 loss: 0.04149685800075531\n",
      "Iteration 4050 loss: 0.04148770868778229\n",
      "Iteration 4060 loss: 0.041478563100099564\n",
      "Iteration 4070 loss: 0.04146941751241684\n",
      "Iteration 4080 loss: 0.04146026074886322\n",
      "Iteration 4090 loss: 0.041451115161180496\n",
      "Iteration 4100 loss: 0.04144196957349777\n",
      "Iteration 4110 loss: 0.041432809084653854\n",
      "Iteration 4120 loss: 0.04142365977168083\n",
      "Iteration 4130 loss: 0.04141451418399811\n",
      "Iteration 4140 loss: 0.041405364871025085\n",
      "Iteration 4150 loss: 0.04139619320631027\n",
      "Iteration 4160 loss: 0.04138704389333725\n",
      "Iteration 4170 loss: 0.04137788712978363\n",
      "Iteration 4180 loss: 0.04136873409152031\n",
      "Iteration 4190 loss: 0.04135957360267639\n",
      "Iteration 4200 loss: 0.041350413113832474\n",
      "Iteration 4210 loss: 0.04134124889969826\n",
      "Iteration 4220 loss: 0.04133209213614464\n",
      "Iteration 4230 loss: 0.04132293164730072\n",
      "Iteration 4240 loss: 0.041313763707876205\n",
      "Iteration 4250 loss: 0.04130460321903229\n",
      "Iteration 4260 loss: 0.04129544273018837\n",
      "Iteration 4270 loss: 0.04128628596663475\n",
      "Iteration 4280 loss: 0.04127711430191994\n",
      "Iteration 4290 loss: 0.041267938911914825\n",
      "Iteration 4300 loss: 0.04125877469778061\n",
      "Iteration 4310 loss: 0.04124961048364639\n",
      "Iteration 4320 loss: 0.04124043881893158\n",
      "Iteration 4330 loss: 0.041231270879507065\n",
      "Iteration 4340 loss: 0.04122209548950195\n",
      "Iteration 4350 loss: 0.04121292009949684\n",
      "Iteration 4360 loss: 0.04120374843478203\n",
      "Iteration 4370 loss: 0.04119458049535751\n",
      "Iteration 4380 loss: 0.041185397654771805\n",
      "Iteration 4390 loss: 0.04117622599005699\n",
      "Iteration 4400 loss: 0.04116705805063248\n",
      "Iteration 4410 loss: 0.041157860308885574\n",
      "Iteration 4420 loss: 0.04114869236946106\n",
      "Iteration 4430 loss: 0.04113951325416565\n",
      "Iteration 4440 loss: 0.041130319237709045\n",
      "Iteration 4450 loss: 0.04112113639712334\n",
      "Iteration 4460 loss: 0.04111195355653763\n",
      "Iteration 4470 loss: 0.04110277444124222\n",
      "Iteration 4480 loss: 0.041093580424785614\n",
      "Iteration 4490 loss: 0.041084397584199905\n",
      "Iteration 4500 loss: 0.0410752035677433\n",
      "Iteration 4510 loss: 0.041066017001867294\n",
      "Iteration 4520 loss: 0.04105682671070099\n",
      "Iteration 4530 loss: 0.04104764014482498\n",
      "Iteration 4540 loss: 0.04103844612836838\n",
      "Iteration 4550 loss: 0.041029252111911774\n",
      "Iteration 4560 loss: 0.04102006182074547\n",
      "Iteration 4570 loss: 0.04101085290312767\n",
      "Iteration 4580 loss: 0.041001662611961365\n",
      "Iteration 4590 loss: 0.04099244996905327\n",
      "Iteration 4600 loss: 0.04098326340317726\n",
      "Iteration 4610 loss: 0.04097404703497887\n",
      "Iteration 4620 loss: 0.04096485674381256\n",
      "Iteration 4630 loss: 0.04095564782619476\n",
      "Iteration 4640 loss: 0.040946442633867264\n",
      "Iteration 4650 loss: 0.040937233716249466\n",
      "Iteration 4660 loss: 0.04092802852392197\n",
      "Iteration 4670 loss: 0.04091881215572357\n",
      "Iteration 4680 loss: 0.040909599512815475\n",
      "Iteration 4690 loss: 0.04090040549635887\n",
      "Iteration 4700 loss: 0.04089117422699928\n",
      "Iteration 4710 loss: 0.04088311269879341\n",
      "Iteration 4720 loss: 0.04087274894118309\n",
      "Iteration 4730 loss: 0.040863536298274994\n",
      "Iteration 4740 loss: 0.0408543199300766\n",
      "Iteration 4750 loss: 0.04084509238600731\n",
      "Iteration 4760 loss: 0.040835872292518616\n",
      "Iteration 4770 loss: 0.04082665219902992\n",
      "Iteration 4780 loss: 0.040817420929670334\n",
      "Iteration 4790 loss: 0.040808193385601044\n",
      "Iteration 4800 loss: 0.04079897701740265\n",
      "Iteration 4810 loss: 0.04078973829746246\n",
      "Iteration 4820 loss: 0.04078051820397377\n",
      "Iteration 4830 loss: 0.04077128320932388\n",
      "Iteration 4840 loss: 0.040762051939964294\n",
      "Iteration 4850 loss: 0.040752824395895004\n",
      "Iteration 4860 loss: 0.04074358567595482\n",
      "Iteration 4870 loss: 0.04073434695601463\n",
      "Iteration 4880 loss: 0.04072510823607445\n",
      "Iteration 4890 loss: 0.04071586951613426\n",
      "Iteration 4900 loss: 0.04070661962032318\n",
      "Iteration 4910 loss: 0.040697384625673294\n",
      "Iteration 4920 loss: 0.04068814218044281\n",
      "Iteration 4930 loss: 0.04067888855934143\n",
      "Iteration 4940 loss: 0.04066964611411095\n",
      "Iteration 4950 loss: 0.04066040366888046\n",
      "Iteration 4960 loss: 0.040651146322488785\n",
      "Iteration 4970 loss: 0.0406419038772583\n",
      "Iteration 4980 loss: 0.04063265770673752\n",
      "Iteration 4990 loss: 0.04062339663505554\n",
      "Iteration 5000 loss: 0.040614135563373566\n",
      "Iteration 5010 loss: 0.040604881942272186\n",
      "Iteration 5020 loss: 0.04059562087059021\n",
      "Iteration 5030 loss: 0.04058636352419853\n",
      "Iteration 5040 loss: 0.040577102452516556\n",
      "Iteration 5050 loss: 0.04056784138083458\n",
      "Iteration 5060 loss: 0.0405585840344429\n",
      "Iteration 5070 loss: 0.04054930433630943\n",
      "Iteration 5080 loss: 0.040540050715208054\n",
      "Iteration 5090 loss: 0.04053077846765518\n",
      "Iteration 5100 loss: 0.04052150994539261\n",
      "Iteration 5110 loss: 0.04051223397254944\n",
      "Iteration 5120 loss: 0.04050295427441597\n",
      "Iteration 5130 loss: 0.0404936783015728\n",
      "Iteration 5140 loss: 0.040484413504600525\n",
      "Iteration 5150 loss: 0.040475137531757355\n",
      "Iteration 5160 loss: 0.040465857833623886\n",
      "Iteration 5170 loss: 0.040456563234329224\n",
      "Iteration 5180 loss: 0.04044729471206665\n",
      "Iteration 5190 loss: 0.04043802246451378\n",
      "Iteration 5200 loss: 0.04042871296405792\n",
      "Iteration 5210 loss: 0.04041944071650505\n",
      "Iteration 5220 loss: 0.04041014611721039\n",
      "Iteration 5230 loss: 0.04040085896849632\n",
      "Iteration 5240 loss: 0.04039155691862106\n",
      "Iteration 5250 loss: 0.0403822660446167\n",
      "Iteration 5260 loss: 0.040372975170612335\n",
      "Iteration 5270 loss: 0.040363673120737076\n",
      "Iteration 5280 loss: 0.04035438597202301\n",
      "Iteration 5290 loss: 0.04034508392214775\n",
      "Iteration 5300 loss: 0.04033578932285309\n",
      "Iteration 5310 loss: 0.04032647982239723\n",
      "Iteration 5320 loss: 0.040317174047231674\n",
      "Iteration 5330 loss: 0.040307871997356415\n",
      "Iteration 5340 loss: 0.040298569947481155\n",
      "Iteration 5350 loss: 0.0402892641723156\n",
      "Iteration 5360 loss: 0.04027995094656944\n",
      "Iteration 5370 loss: 0.04027063772082329\n",
      "Iteration 5380 loss: 0.040261320769786835\n",
      "Iteration 5390 loss: 0.04025201126933098\n",
      "Iteration 5400 loss: 0.040242698043584824\n",
      "Iteration 5410 loss: 0.040233369916677475\n",
      "Iteration 5420 loss: 0.04022406041622162\n",
      "Iteration 5430 loss: 0.04021472856402397\n",
      "Iteration 5440 loss: 0.04020541161298752\n",
      "Iteration 5450 loss: 0.04019609093666077\n",
      "Iteration 5460 loss: 0.040186766535043716\n",
      "Iteration 5470 loss: 0.04017743840813637\n",
      "Iteration 5480 loss: 0.04016811400651932\n",
      "Iteration 5490 loss: 0.04015878215432167\n",
      "Iteration 5500 loss: 0.04014945030212402\n",
      "Iteration 5510 loss: 0.040140118449926376\n",
      "Iteration 5520 loss: 0.04013077914714813\n",
      "Iteration 5530 loss: 0.04012144356966019\n",
      "Iteration 5540 loss: 0.04011210799217224\n",
      "Iteration 5550 loss: 0.040102776139974594\n",
      "Iteration 5560 loss: 0.040093425661325455\n",
      "Iteration 5570 loss: 0.04008408635854721\n",
      "Iteration 5580 loss: 0.04007473960518837\n",
      "Iteration 5590 loss: 0.040065400302410126\n",
      "Iteration 5600 loss: 0.040056053549051285\n",
      "Iteration 5610 loss: 0.040046706795692444\n",
      "Iteration 5620 loss: 0.040037352591753006\n",
      "Iteration 5630 loss: 0.04002801328897476\n",
      "Iteration 5640 loss: 0.04001864045858383\n",
      "Iteration 5650 loss: 0.04000930115580559\n",
      "Iteration 5660 loss: 0.03999993950128555\n",
      "Iteration 5670 loss: 0.03999227657914162\n",
      "Iteration 5680 loss: 0.03998122364282608\n",
      "Iteration 5690 loss: 0.039971861988306046\n",
      "Iteration 5700 loss: 0.03996250033378601\n",
      "Iteration 5710 loss: 0.03995315358042717\n",
      "Iteration 5720 loss: 0.039943769574165344\n",
      "Iteration 5730 loss: 0.039934396743774414\n",
      "Iteration 5740 loss: 0.03992503881454468\n",
      "Iteration 5750 loss: 0.039915669709444046\n",
      "Iteration 5760 loss: 0.03990629315376282\n",
      "Iteration 5770 loss: 0.03989692032337189\n",
      "Iteration 5780 loss: 0.039887551218271255\n",
      "Iteration 5790 loss: 0.03987817466259003\n",
      "Iteration 5800 loss: 0.0398687906563282\n",
      "Iteration 5810 loss: 0.039859429001808167\n",
      "Iteration 5820 loss: 0.03985004499554634\n",
      "Iteration 5830 loss: 0.03984065726399422\n",
      "Iteration 5840 loss: 0.03983127698302269\n",
      "Iteration 5850 loss: 0.03982188552618027\n",
      "Iteration 5860 loss: 0.039812516421079636\n",
      "Iteration 5870 loss: 0.03980312496423721\n",
      "Iteration 5880 loss: 0.03979373350739479\n",
      "Iteration 5890 loss: 0.03978434577584267\n",
      "Iteration 5900 loss: 0.03977495804429054\n",
      "Iteration 5910 loss: 0.03976556286215782\n",
      "Iteration 5920 loss: 0.0397561751306057\n",
      "Iteration 5930 loss: 0.03974677622318268\n",
      "Iteration 5940 loss: 0.03973736613988876\n",
      "Iteration 5950 loss: 0.03972797095775604\n",
      "Iteration 5960 loss: 0.03971857950091362\n",
      "Iteration 5970 loss: 0.0397091768682003\n",
      "Iteration 5980 loss: 0.03969977796077728\n",
      "Iteration 5990 loss: 0.039690371602773666\n",
      "Iteration 6000 loss: 0.03968096524477005\n",
      "Iteration 6010 loss: 0.03967156261205673\n",
      "Iteration 6020 loss: 0.039662156254053116\n",
      "Iteration 6030 loss: 0.0396527498960495\n",
      "Iteration 6040 loss: 0.03964334353804588\n",
      "Iteration 6050 loss: 0.03963392972946167\n",
      "Iteration 6060 loss: 0.03962450847029686\n",
      "Iteration 6070 loss: 0.03961510211229324\n",
      "Iteration 6080 loss: 0.03960568830370903\n",
      "Iteration 6090 loss: 0.03959625959396362\n",
      "Iteration 6100 loss: 0.03958685323596001\n",
      "Iteration 6110 loss: 0.0395774245262146\n",
      "Iteration 6120 loss: 0.039568010717630386\n",
      "Iteration 6130 loss: 0.03955858573317528\n",
      "Iteration 6140 loss: 0.03954915329813957\n",
      "Iteration 6150 loss: 0.03953973576426506\n",
      "Iteration 6160 loss: 0.03953031077980995\n",
      "Iteration 6170 loss: 0.03952087461948395\n",
      "Iteration 6180 loss: 0.03951144963502884\n",
      "Iteration 6190 loss: 0.03950202092528343\n",
      "Iteration 6200 loss: 0.03949259594082832\n",
      "Iteration 6210 loss: 0.039483170956373215\n",
      "Iteration 6220 loss: 0.039473727345466614\n",
      "Iteration 6230 loss: 0.03946429491043091\n",
      "Iteration 6240 loss: 0.0394548624753952\n",
      "Iteration 6250 loss: 0.0394454300403595\n",
      "Iteration 6260 loss: 0.03943599388003349\n",
      "Iteration 6270 loss: 0.039426542818546295\n",
      "Iteration 6280 loss: 0.03941710293292999\n",
      "Iteration 6290 loss: 0.03940767049789429\n",
      "Iteration 6300 loss: 0.03939821943640709\n",
      "Iteration 6310 loss: 0.039388783276081085\n",
      "Iteration 6320 loss: 0.03937933221459389\n",
      "Iteration 6330 loss: 0.039369888603687286\n",
      "Iteration 6340 loss: 0.03936044126749039\n",
      "Iteration 6350 loss: 0.03935099393129349\n",
      "Iteration 6360 loss: 0.03934154659509659\n",
      "Iteration 6370 loss: 0.03933210298418999\n",
      "Iteration 6380 loss: 0.03932265192270279\n",
      "Iteration 6390 loss: 0.0393131822347641\n",
      "Iteration 6400 loss: 0.039303746074438095\n",
      "Iteration 6410 loss: 0.0392942801117897\n",
      "Iteration 6420 loss: 0.039284832775592804\n",
      "Iteration 6430 loss: 0.03927537053823471\n",
      "Iteration 6440 loss: 0.03926591947674751\n",
      "Iteration 6450 loss: 0.03925646096467972\n",
      "Iteration 6460 loss: 0.039246995002031326\n",
      "Iteration 6470 loss: 0.03923754021525383\n",
      "Iteration 6480 loss: 0.03922807425260544\n",
      "Iteration 6490 loss: 0.039218612015247345\n",
      "Iteration 6500 loss: 0.03920914605259895\n",
      "Iteration 6510 loss: 0.0392003059387207\n",
      "Iteration 6520 loss: 0.03919021412730217\n",
      "Iteration 6530 loss: 0.03918074816465378\n",
      "Iteration 6540 loss: 0.03917129337787628\n",
      "Iteration 6550 loss: 0.03916183114051819\n",
      "Iteration 6560 loss: 0.039152346551418304\n",
      "Iteration 6570 loss: 0.03914288058876991\n",
      "Iteration 6580 loss: 0.03913341462612152\n",
      "Iteration 6590 loss: 0.03912394121289253\n",
      "Iteration 6600 loss: 0.039114464074373245\n",
      "Iteration 6610 loss: 0.039104994386434555\n",
      "Iteration 6620 loss: 0.03909552842378616\n",
      "Iteration 6630 loss: 0.039086051285266876\n",
      "Iteration 6640 loss: 0.039076585322618484\n",
      "Iteration 6650 loss: 0.039067093282938004\n",
      "Iteration 6660 loss: 0.03905762359499931\n",
      "Iteration 6670 loss: 0.039048146456480026\n",
      "Iteration 6680 loss: 0.03903866559267044\n",
      "Iteration 6690 loss: 0.03902919590473175\n",
      "Iteration 6700 loss: 0.03901970013976097\n",
      "Iteration 6710 loss: 0.03901023417711258\n",
      "Iteration 6720 loss: 0.03900075703859329\n",
      "Iteration 6730 loss: 0.03899126127362251\n",
      "Iteration 6740 loss: 0.03898179158568382\n",
      "Iteration 6750 loss: 0.03897229582071304\n",
      "Iteration 6760 loss: 0.03896281495690346\n",
      "Iteration 6770 loss: 0.03895333409309387\n",
      "Iteration 6780 loss: 0.03894384950399399\n",
      "Iteration 6790 loss: 0.038934361189603806\n",
      "Iteration 6800 loss: 0.03892487287521362\n",
      "Iteration 6810 loss: 0.03891538083553314\n",
      "Iteration 6820 loss: 0.038905903697013855\n",
      "Iteration 6830 loss: 0.03889641910791397\n",
      "Iteration 6840 loss: 0.03888691961765289\n",
      "Iteration 6850 loss: 0.03887743502855301\n",
      "Iteration 6860 loss: 0.038867950439453125\n",
      "Iteration 6870 loss: 0.03885846212506294\n",
      "Iteration 6880 loss: 0.03884897381067276\n",
      "Iteration 6890 loss: 0.03883947432041168\n",
      "Iteration 6900 loss: 0.038829997181892395\n",
      "Iteration 6910 loss: 0.03882049396634102\n",
      "Iteration 6920 loss: 0.038811005651950836\n",
      "Iteration 6930 loss: 0.038801513612270355\n",
      "Iteration 6940 loss: 0.038792021572589874\n",
      "Iteration 6950 loss: 0.038782522082328796\n",
      "Iteration 6960 loss: 0.038773030042648315\n",
      "Iteration 6970 loss: 0.038763534277677536\n",
      "Iteration 6980 loss: 0.03875403851270676\n",
      "Iteration 6990 loss: 0.03874455392360687\n",
      "Iteration 7000 loss: 0.03873505815863609\n",
      "Iteration 7010 loss: 0.038725558668375015\n",
      "Iteration 7020 loss: 0.038716062903404236\n",
      "Iteration 7030 loss: 0.03870657458901405\n",
      "Iteration 7040 loss: 0.03869706764817238\n",
      "Iteration 7050 loss: 0.038687564432621\n",
      "Iteration 7060 loss: 0.03867807611823082\n",
      "Iteration 7070 loss: 0.03866858035326004\n",
      "Iteration 7080 loss: 0.03865908831357956\n",
      "Iteration 7090 loss: 0.03864959254860878\n",
      "Iteration 7100 loss: 0.03864008188247681\n",
      "Iteration 7110 loss: 0.038630593568086624\n",
      "Iteration 7120 loss: 0.03862110525369644\n",
      "Iteration 7130 loss: 0.03861161321401596\n",
      "Iteration 7140 loss: 0.038602109998464584\n",
      "Iteration 7150 loss: 0.038592610508203506\n",
      "Iteration 7160 loss: 0.03858311101794243\n",
      "Iteration 7170 loss: 0.03857360780239105\n",
      "Iteration 7180 loss: 0.03856411948800087\n",
      "Iteration 7190 loss: 0.03855461627244949\n",
      "Iteration 7200 loss: 0.03854512423276901\n",
      "Iteration 7210 loss: 0.038535621017217636\n",
      "Iteration 7220 loss: 0.038526128977537155\n",
      "Iteration 7230 loss: 0.03851662576198578\n",
      "Iteration 7240 loss: 0.0385071337223053\n",
      "Iteration 7250 loss: 0.03849763050675392\n",
      "Iteration 7260 loss: 0.038488131016492844\n",
      "Iteration 7270 loss: 0.03847863897681236\n",
      "Iteration 7280 loss: 0.038469139486551285\n",
      "Iteration 7290 loss: 0.0384596511721611\n",
      "Iteration 7300 loss: 0.03845015540719032\n",
      "Iteration 7310 loss: 0.038440655916929245\n",
      "Iteration 7320 loss: 0.03843115642666817\n",
      "Iteration 7330 loss: 0.038421668112277985\n",
      "Iteration 7340 loss: 0.03841216862201691\n",
      "Iteration 7350 loss: 0.03840267285704613\n",
      "Iteration 7360 loss: 0.038393184542655945\n",
      "Iteration 7370 loss: 0.03838368505239487\n",
      "Iteration 7380 loss: 0.038374196738004684\n",
      "Iteration 7390 loss: 0.038364700973033905\n",
      "Iteration 7400 loss: 0.03835519403219223\n",
      "Iteration 7410 loss: 0.038345709443092346\n",
      "Iteration 7420 loss: 0.038336217403411865\n",
      "Iteration 7430 loss: 0.03832671791315079\n",
      "Iteration 7440 loss: 0.0383172370493412\n",
      "Iteration 7450 loss: 0.03830774128437042\n",
      "Iteration 7460 loss: 0.03829825296998024\n",
      "Iteration 7470 loss: 0.03828876093029976\n",
      "Iteration 7480 loss: 0.03827926516532898\n",
      "Iteration 7490 loss: 0.0382697768509388\n",
      "Iteration 7500 loss: 0.03826029226183891\n",
      "Iteration 7510 loss: 0.03825080394744873\n",
      "Iteration 7520 loss: 0.03824131563305855\n",
      "Iteration 7530 loss: 0.038231827318668365\n",
      "Iteration 7540 loss: 0.03822234272956848\n",
      "Iteration 7550 loss: 0.03821286931633949\n",
      "Iteration 7560 loss: 0.03820337355136871\n",
      "Iteration 7570 loss: 0.03819388896226883\n",
      "Iteration 7580 loss: 0.03818441182374954\n",
      "Iteration 7590 loss: 0.03817492350935936\n",
      "Iteration 7600 loss: 0.038165442645549774\n",
      "Iteration 7610 loss: 0.03815596178174019\n",
      "Iteration 7620 loss: 0.0381464883685112\n",
      "Iteration 7630 loss: 0.03813701122999191\n",
      "Iteration 7640 loss: 0.038127511739730835\n",
      "Iteration 7650 loss: 0.038118038326501846\n",
      "Iteration 7660 loss: 0.03810856491327286\n",
      "Iteration 7670 loss: 0.03809909150004387\n",
      "Iteration 7680 loss: 0.03808961436152458\n",
      "Iteration 7690 loss: 0.038080137223005295\n",
      "Iteration 7700 loss: 0.038070663809776306\n",
      "Iteration 7710 loss: 0.038061194121837616\n",
      "Iteration 7720 loss: 0.03805171325802803\n",
      "Iteration 7730 loss: 0.03804224357008934\n",
      "Iteration 7740 loss: 0.038032785058021545\n",
      "Iteration 7750 loss: 0.03802332282066345\n",
      "Iteration 7760 loss: 0.03801383823156357\n",
      "Iteration 7770 loss: 0.038004372268915176\n",
      "Iteration 7780 loss: 0.03799491748213768\n",
      "Iteration 7790 loss: 0.037985462695360184\n",
      "Iteration 7800 loss: 0.037975989282131195\n",
      "Iteration 7810 loss: 0.037966519594192505\n",
      "Iteration 7820 loss: 0.03795706480741501\n",
      "Iteration 7830 loss: 0.037947602570056915\n",
      "Iteration 7840 loss: 0.03793814405798912\n",
      "Iteration 7850 loss: 0.03792869672179222\n",
      "Iteration 7860 loss: 0.03791923075914383\n",
      "Iteration 7870 loss: 0.03790977597236633\n",
      "Iteration 7880 loss: 0.03790033236145973\n",
      "Iteration 7890 loss: 0.03789087384939194\n",
      "Iteration 7900 loss: 0.03788142651319504\n",
      "Iteration 7910 loss: 0.037871964275836945\n",
      "Iteration 7920 loss: 0.03786252439022064\n",
      "Iteration 7930 loss: 0.03785307705402374\n",
      "Iteration 7940 loss: 0.03784364089369774\n",
      "Iteration 7950 loss: 0.03783418610692024\n",
      "Iteration 7960 loss: 0.03782475367188454\n",
      "Iteration 7970 loss: 0.03781530633568764\n",
      "Iteration 7980 loss: 0.03780587390065193\n",
      "Iteration 7990 loss: 0.03779643401503563\n",
      "Iteration 8000 loss: 0.03778700530529022\n",
      "Iteration 8010 loss: 0.03777757287025452\n",
      "Iteration 8020 loss: 0.037768132984638214\n",
      "Iteration 8030 loss: 0.03775870054960251\n",
      "Iteration 8040 loss: 0.0377492792904377\n",
      "Iteration 8050 loss: 0.03773985430598259\n",
      "Iteration 8060 loss: 0.037730421870946884\n",
      "Iteration 8070 loss: 0.037721000611782074\n",
      "Iteration 8080 loss: 0.037711575627326965\n",
      "Iteration 8090 loss: 0.037702158093452454\n",
      "Iteration 8100 loss: 0.03769273683428764\n",
      "Iteration 8110 loss: 0.037683311849832535\n",
      "Iteration 8120 loss: 0.037673912942409515\n",
      "Iteration 8130 loss: 0.0376644991338253\n",
      "Iteration 8140 loss: 0.03765508532524109\n",
      "Iteration 8150 loss: 0.037645671516656876\n",
      "Iteration 8160 loss: 0.03763626888394356\n",
      "Iteration 8170 loss: 0.03762685880064964\n",
      "Iteration 8180 loss: 0.03761746734380722\n",
      "Iteration 8190 loss: 0.037608057260513306\n",
      "Iteration 8200 loss: 0.03759865462779999\n",
      "Iteration 8210 loss: 0.03758927062153816\n",
      "Iteration 8220 loss: 0.03757987916469574\n",
      "Iteration 8230 loss: 0.037570469081401825\n",
      "Iteration 8240 loss: 0.0375610776245594\n",
      "Iteration 8250 loss: 0.037551697343587875\n",
      "Iteration 8260 loss: 0.03754230961203575\n",
      "Iteration 8270 loss: 0.03753292188048363\n",
      "Iteration 8280 loss: 0.0375235415995121\n",
      "Iteration 8290 loss: 0.037514157593250275\n",
      "Iteration 8300 loss: 0.03750479593873024\n",
      "Iteration 8310 loss: 0.037495408207178116\n",
      "Iteration 8320 loss: 0.03748603165149689\n",
      "Iteration 8330 loss: 0.037476662546396255\n",
      "Iteration 8340 loss: 0.03746729716658592\n",
      "Iteration 8350 loss: 0.03745793551206589\n",
      "Iteration 8360 loss: 0.037448566406965256\n",
      "Iteration 8370 loss: 0.03743920102715492\n",
      "Iteration 8380 loss: 0.037429843097925186\n",
      "Iteration 8390 loss: 0.03742048889398575\n",
      "Iteration 8400 loss: 0.037411127239465714\n",
      "Iteration 8410 loss: 0.03740178421139717\n",
      "Iteration 8420 loss: 0.03739244118332863\n",
      "Iteration 8430 loss: 0.037383098155260086\n",
      "Iteration 8440 loss: 0.03737374022603035\n",
      "Iteration 8450 loss: 0.0373644083738327\n",
      "Iteration 8460 loss: 0.037355076521635056\n",
      "Iteration 8470 loss: 0.037345729768276215\n",
      "Iteration 8480 loss: 0.03733639791607857\n",
      "Iteration 8490 loss: 0.03732706233859062\n",
      "Iteration 8500 loss: 0.03731773421168327\n",
      "Iteration 8510 loss: 0.03730841353535652\n",
      "Iteration 8520 loss: 0.03729908540844917\n",
      "Iteration 8530 loss: 0.03728976845741272\n",
      "Iteration 8540 loss: 0.03728046268224716\n",
      "Iteration 8550 loss: 0.03727114200592041\n",
      "Iteration 8560 loss: 0.037261828780174255\n",
      "Iteration 8570 loss: 0.037252526730298996\n",
      "Iteration 8580 loss: 0.037243206053972244\n",
      "Iteration 8590 loss: 0.03723391145467758\n",
      "Iteration 8600 loss: 0.03722461313009262\n",
      "Iteration 8610 loss: 0.03721531853079796\n",
      "Iteration 8620 loss: 0.037206020206213\n",
      "Iteration 8630 loss: 0.03719673305749893\n",
      "Iteration 8640 loss: 0.03718743845820427\n",
      "Iteration 8650 loss: 0.037178173661231995\n",
      "Iteration 8660 loss: 0.03716889023780823\n",
      "Iteration 8670 loss: 0.03715961053967476\n",
      "Iteration 8680 loss: 0.03715032711625099\n",
      "Iteration 8690 loss: 0.03714105859398842\n",
      "Iteration 8700 loss: 0.037131790071725845\n",
      "Iteration 8710 loss: 0.03712252527475357\n",
      "Iteration 8720 loss: 0.03711326792836189\n",
      "Iteration 8730 loss: 0.037104006856679916\n",
      "Iteration 8740 loss: 0.03709475323557854\n",
      "Iteration 8750 loss: 0.03708551451563835\n",
      "Iteration 8760 loss: 0.037076257169246674\n",
      "Iteration 8770 loss: 0.03706701099872589\n",
      "Iteration 8780 loss: 0.03705776855349541\n",
      "Iteration 8790 loss: 0.037048518657684326\n",
      "Iteration 8800 loss: 0.03703928738832474\n",
      "Iteration 8810 loss: 0.037030063569545746\n",
      "Iteration 8820 loss: 0.037020836025476456\n",
      "Iteration 8830 loss: 0.037011608481407166\n",
      "Iteration 8840 loss: 0.03700239211320877\n",
      "Iteration 8850 loss: 0.036993175745010376\n",
      "Iteration 8860 loss: 0.03698396310210228\n",
      "Iteration 8870 loss: 0.03697475045919418\n",
      "Iteration 8880 loss: 0.03696553409099579\n",
      "Iteration 8890 loss: 0.03695634379982948\n",
      "Iteration 8900 loss: 0.03694714233279228\n",
      "Iteration 8910 loss: 0.036937955766916275\n",
      "Iteration 8920 loss: 0.036928750574588776\n",
      "Iteration 8930 loss: 0.03691956773400307\n",
      "Iteration 8940 loss: 0.03691038861870766\n",
      "Iteration 8950 loss: 0.036901213228702545\n",
      "Iteration 8960 loss: 0.03689203038811684\n",
      "Iteration 8970 loss: 0.03688285872340202\n",
      "Iteration 8980 loss: 0.03687368333339691\n",
      "Iteration 8990 loss: 0.036864522844552994\n",
      "Iteration 9000 loss: 0.03685535863041878\n",
      "Iteration 9010 loss: 0.03684621676802635\n",
      "Iteration 9020 loss: 0.03683704882860184\n",
      "Iteration 9030 loss: 0.03682791441679001\n",
      "Iteration 9040 loss: 0.03681876137852669\n",
      "Iteration 9050 loss: 0.03680962324142456\n",
      "Iteration 9060 loss: 0.03680047765374184\n",
      "Iteration 9070 loss: 0.0367913618683815\n",
      "Iteration 9080 loss: 0.03678222373127937\n",
      "Iteration 9090 loss: 0.03677310049533844\n",
      "Iteration 9100 loss: 0.036763984709978104\n",
      "Iteration 9110 loss: 0.03675486519932747\n",
      "Iteration 9120 loss: 0.03674575313925743\n",
      "Iteration 9130 loss: 0.03673664852976799\n",
      "Iteration 9140 loss: 0.03672754392027855\n",
      "Iteration 9150 loss: 0.0367184579372406\n",
      "Iteration 9160 loss: 0.03670935332775116\n",
      "Iteration 9170 loss: 0.03670026361942291\n",
      "Iteration 9180 loss: 0.03669118136167526\n",
      "Iteration 9190 loss: 0.03668211027979851\n",
      "Iteration 9200 loss: 0.03667302802205086\n",
      "Iteration 9210 loss: 0.0366639569401741\n",
      "Iteration 9220 loss: 0.036654889583587646\n",
      "Iteration 9230 loss: 0.03664582595229149\n",
      "Iteration 9240 loss: 0.03663676977157593\n",
      "Iteration 9250 loss: 0.03662770986557007\n",
      "Iteration 9260 loss: 0.036618661135435104\n",
      "Iteration 9270 loss: 0.03660961985588074\n",
      "Iteration 9280 loss: 0.03660057112574577\n",
      "Iteration 9290 loss: 0.0365915484726429\n",
      "Iteration 9300 loss: 0.03658251091837883\n",
      "Iteration 9310 loss: 0.03657348453998566\n",
      "Iteration 9320 loss: 0.03656446561217308\n",
      "Iteration 9330 loss: 0.0365554541349411\n",
      "Iteration 9340 loss: 0.036546435207128525\n",
      "Iteration 9350 loss: 0.036537423729896545\n",
      "Iteration 9360 loss: 0.03652843087911606\n",
      "Iteration 9370 loss: 0.03651943430304527\n",
      "Iteration 9380 loss: 0.03651043772697449\n",
      "Iteration 9390 loss: 0.0365014523267746\n",
      "Iteration 9400 loss: 0.036492474377155304\n",
      "Iteration 9410 loss: 0.03648349270224571\n",
      "Iteration 9420 loss: 0.03647452965378761\n",
      "Iteration 9430 loss: 0.03646555170416832\n",
      "Iteration 9440 loss: 0.03645659238100052\n",
      "Iteration 9450 loss: 0.03644763305783272\n",
      "Iteration 9460 loss: 0.036438677459955215\n",
      "Iteration 9470 loss: 0.03642973303794861\n",
      "Iteration 9480 loss: 0.0364207960665226\n",
      "Iteration 9490 loss: 0.03641185909509659\n",
      "Iteration 9500 loss: 0.036402925848960876\n",
      "Iteration 9510 loss: 0.03639399632811546\n",
      "Iteration 9520 loss: 0.03638508543372154\n",
      "Iteration 9530 loss: 0.036376163363456726\n",
      "Iteration 9540 loss: 0.03636724874377251\n",
      "Iteration 9550 loss: 0.03635834902524948\n",
      "Iteration 9560 loss: 0.036349453032016754\n",
      "Iteration 9570 loss: 0.03634054586291313\n",
      "Iteration 9580 loss: 0.0363316610455513\n",
      "Iteration 9590 loss: 0.03632277622818947\n",
      "Iteration 9600 loss: 0.036313898861408234\n",
      "Iteration 9610 loss: 0.036305032670497894\n",
      "Iteration 9620 loss: 0.03629615530371666\n",
      "Iteration 9630 loss: 0.03628730773925781\n",
      "Iteration 9640 loss: 0.03627844527363777\n",
      "Iteration 9650 loss: 0.03626959025859833\n",
      "Iteration 9660 loss: 0.03626074641942978\n",
      "Iteration 9670 loss: 0.03625190258026123\n",
      "Iteration 9680 loss: 0.03624305874109268\n",
      "Iteration 9690 loss: 0.036234237253665924\n",
      "Iteration 9700 loss: 0.036225419491529465\n",
      "Iteration 9710 loss: 0.036216601729393005\n",
      "Iteration 9720 loss: 0.03620779886841774\n",
      "Iteration 9730 loss: 0.03619898483157158\n",
      "Iteration 9740 loss: 0.03619018569588661\n",
      "Iteration 9750 loss: 0.03618139401078224\n",
      "Iteration 9760 loss: 0.036172594875097275\n",
      "Iteration 9770 loss: 0.0361638180911541\n",
      "Iteration 9780 loss: 0.036155037581920624\n",
      "Iteration 9790 loss: 0.036146268248558044\n",
      "Iteration 9800 loss: 0.036137502640485764\n",
      "Iteration 9810 loss: 0.03612873703241348\n",
      "Iteration 9820 loss: 0.036119990050792694\n",
      "Iteration 9830 loss: 0.03611123934388161\n",
      "Iteration 9840 loss: 0.036102499812841415\n",
      "Iteration 9850 loss: 0.03609376773238182\n",
      "Iteration 9860 loss: 0.036085035651922226\n",
      "Iteration 9870 loss: 0.03607631474733353\n",
      "Iteration 9880 loss: 0.03606759011745453\n",
      "Iteration 9890 loss: 0.03605888411402702\n",
      "Iteration 9900 loss: 0.036050181835889816\n",
      "Iteration 9910 loss: 0.03604147583246231\n",
      "Iteration 9920 loss: 0.036032784730196\n",
      "Iteration 9930 loss: 0.036024097353219986\n",
      "Iteration 9940 loss: 0.03601542487740517\n",
      "Iteration 9950 loss: 0.03600675240159035\n",
      "Iteration 9960 loss: 0.03599807247519493\n",
      "Iteration 9970 loss: 0.035989414900541306\n",
      "Iteration 9980 loss: 0.03598076105117798\n",
      "Iteration 9990 loss: 0.03597210347652435\n"
     ]
    }
   ],
   "source": [
    "params = torch.tensor([0.5,0.5,0.5, 0.5], dtype=torch.float32)\n",
    "params, losses = optimize_params(params, cut[0], X, lr = 0.01, n_iter = 10000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x2426474b370>]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.legend.Legend at 0x24269e11f30>"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now lets compare\n",
    "plt.plot(X,cut[0], label=\"Original\")\n",
    "\n",
    "# We sample 1000 times and take the mean\n",
    "means = params[0].repeat(X.shape[0])\n",
    "cov = cov_kernel(X,X[...,None],params[1:])\n",
    "\n",
    "S = tdb.MultivariateNormal(means,cov).sample(sample_shape=torch.Size([1000])).mean(0)\n",
    "plt.plot(X,S, label=\"Sampled\")\n",
    "\n",
    "plt.title(\"Original vs Sampled\")\n",
    "plt.legend()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "Doesn't seem to work as well, lets try with the other method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}